{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process COVID-19 cases per country with data fetched from Delta Lake and SageMaker Processing\n",
    "\n",
    "<b>This notebook was tested on SageMaker Studio with Python 3 (Data Science) Kernel.</b>\n",
    "\n",
    "In this notebook, we provide a detailed walk-through on how to package a scikit-learn Docker image for processing job that fetch data from a table on Delta Lake, and aggregate total COVID-19 cases per country. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role, Session, image_uris\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sagemaker_session = Session()\n",
    "\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"sagemaker/delta-lake-scikit-learn-processing-demo\"\n",
    "\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a `scikit-learn` container for running the preprocessing job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by installing the required package.\n",
    "\n",
    "You can read more about it in this blog post: https://aws.amazon.com/blogs/machine-learning/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker-studio-image-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sm-docker build ./container/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the following output:\n",
    "\n",
    "```\n",
    "Image URI: 0xxxxxxxxxx.dkr.ecr.us-east-1.amazonaws.com/sagemaker-studio-d-xxxxxxxxxxxx:default-1640008606254\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri='<Image URI output from sm-docker build output>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download profile file\n",
    "\n",
    "We will download a profile file for the Delta Sharing Server that Databricks are hosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_file = \"https://raw.githubusercontent.com/delta-io/delta-sharing/main/examples/open-datasets.share\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget {profile_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically this file is managed and secured on the client-side. Because our first experiment with Delta Sharing is about reading data from the Databricks server, we can stick with the provided example profile_file on GitHub and retrieve it via HTTP.\n",
    "\n",
    "To get a better idea of the content and syntax of that file, Let's display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat open-datasets.share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload profile file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_profile_file_url = sagemaker.Session().upload_data(\n",
    "    \"open-datasets.share\", bucket=bucket, key_prefix=prefix + \"/profile\"\n",
    ")\n",
    "\n",
    "print(sample_profile_file_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a processing script\n",
    "\n",
    "This notebook use a file `processing_script.py`, which contains the pre-processing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize processing_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the relevant lines in the training script that create a `SharingClient` and load the table as a Pandas DataFrame:\n",
    "\n",
    "```\n",
    "    profile_file = profile_files[0]\n",
    "    print(f'Found profile file: {profile_file}')\n",
    "\n",
    "    # Create a SharingClient\n",
    "    client = delta_sharing.SharingClient(profile_file)\n",
    "    table_url = profile_file + \"#delta_sharing.default.owid-covid-data\"\n",
    "\n",
    "    # Load the table as a Pandas DataFrame\n",
    "    print('Loading owid-covid-data table from Delta Lake')\n",
    "    data = delta_sharing.load_as_pandas(table_url)\n",
    "    print(f'Data shape: {data.shape}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Processing\n",
    "\n",
    "We will now launch a processing job with the Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ScriptProcessor(command=['python3'],\n",
    "                    image_uri=image_uri,\n",
    "                    role=role,\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.m5.large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.run(code='processing_script.py',\n",
    "                    inputs=[ProcessingInput(\n",
    "                        source=sample_profile_file_url,\n",
    "                        destination='/opt/ml/processing/profile/')],\n",
    "                    outputs=[ProcessingOutput(\n",
    "                        output_name='delta_lake_processed_data',\n",
    "                        source='/opt/ml/processing/processed_data/')]\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the results of the processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_job_description = processor.jobs[-1].describe()\n",
    "output_config = preprocessing_job_description['ProcessingOutputConfig']\n",
    "output_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'delta_lake_processed_data':\n",
    "        delta_lake_processed_data_file = output['S3Output']['S3Uri']\n",
    "        bucket = delta_lake_processed_data_file.split(\"/\")[:3][2]\n",
    "        output_file_name = '/'.join(delta_lake_processed_data_file.split(\"/\")[3:])+\"/total_cases_per_location.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = s3.get_object(Bucket=bucket, Key=output_file_name)\n",
    "content = data['Body'].read()\n",
    "content.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
